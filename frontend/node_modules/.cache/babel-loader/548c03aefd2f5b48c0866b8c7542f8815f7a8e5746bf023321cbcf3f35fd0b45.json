{"ast":null,"code":"export async function sendMsg(message) {\n  const result = await fetch('http://localhost:11434/api/generate', {\n    \"model\": \"llama2\",\n    \"prompt\": message,\n    \"stream\": false\n  });\n  return result.response;\n}","map":{"version":3,"names":["sendMsg","message","result","fetch","response"],"sources":["/Users/sarthaktanwar/Coding/gpt-clone/src/llama.js"],"sourcesContent":["export async function sendMsg(message) {\n    const result = await fetch('http://localhost:11434/api/generate', {\n        \"model\": \"llama2\", \n        \"prompt\": message, \n        \"stream\": false\n    });\n    return result.response;\n}"],"mappings":"AAAA,OAAO,eAAeA,OAAOA,CAACC,OAAO,EAAE;EACnC,MAAMC,MAAM,GAAG,MAAMC,KAAK,CAAC,qCAAqC,EAAE;IAC9D,OAAO,EAAE,QAAQ;IACjB,QAAQ,EAAEF,OAAO;IACjB,QAAQ,EAAE;EACd,CAAC,CAAC;EACF,OAAOC,MAAM,CAACE,QAAQ;AAC1B"},"metadata":{},"sourceType":"module","externalDependencies":[]}